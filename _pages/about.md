---
layout: about
title: me
permalink: /
subtitle: >
    PhD Student in Computer Science at Boston University
    <!-- <p class="motto"> <em> motto </em> </p> -->
profile:
  align: center
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  address: >
    <p> akyurek [at] bu [dot] edu
    <p> 665 Comm Ave, Boston, MA </p>
news: true  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: false  # includes social icons at the bottom of the page
---
I am a fifth-year Computer Science PhD student at Boston University focusing natural language processing. Before that I received my masters in Statistics from Carnegie Mellon University. My current research interests are on leveraging natural language for language model alignment, collaborative language models and ethics in NLP.


**Post-hoc Improvement of Neural Models, Alignment & Model-Editing**


<div class="about-highlight" markdown="1">
- I find it intriguing to devise language models that can critique other language models' outputs. So, I have devised a [critique generator](https://arxiv.org/abs/2305.08844) which is rewarded via reinforcement learning as its critiques improved another model's predictions. Moreover, I am interested in aligning language models via natural language feedback, so I have led the curation of a model editing benchmark [DUnE]() where edits are in natural language.

- I have developed [a scheme](https://arxiv.org/abs/2110.07059) that allows growing the number of a classes that an object classifier can recognize using language information about the objects such as labels and descriptions.
</div>

<p></p>
**Safety in Language Models**


<div class="about-highlight" markdown="1">
- I have studied [bias measurement](https://arxiv.org/abs/2205.11605) in instruction-tuned language models and conducted sensitivity [analysis](https://arxiv.org/abs/2205.11601) for measuring bias in language models.

- A significant portion of our model editing benchmark DUnE includes edits that solicit debiased model outputs. We [find]() that language models struggle parsing the instructions that call for avoiding harmful biases and stereotypes.
</div>




<!-- <h2 style="margin-top: 1rem;">biography</h2>
{%- include_relative bio.md %} -->
